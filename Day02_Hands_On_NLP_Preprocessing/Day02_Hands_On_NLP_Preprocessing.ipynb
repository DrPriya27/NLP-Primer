{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e93c477",
   "metadata": {},
   "source": [
    "# NLP Preprocessing: Step-by-Step Guide\n",
    "\n",
    "This notebook demonstrates various Natural Language Processing (NLP) preprocessing techniques applied to a legal dataset. NLP preprocessing is essential for converting raw text data into a format that can be effectively analyzed by machine learning algorithms.\n",
    "\n",
    "The notebook covers:\n",
    "- Text exploration and basic statistics\n",
    "- Tokenization of text into words\n",
    "- Removal of stopwords\n",
    "- Stemming and lemmatization\n",
    "- Part-of-speech tagging\n",
    "- Text cleaning and normalization\n",
    "- Text visualization\n",
    "- Vectorization techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307ff86b",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "First, we need to import the necessary Python libraries for NLP preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a506b124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag, ne_chunk\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud  # You may need to install this: pip install wordcloud\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Download required NLTK resources\n",
    "print(\"Downloading required NLTK resources...\")\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('maxent_ne_chunker', quiet=True)\n",
    "nltk.download('words', quiet=True)\n",
    "print(\"Downloads complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2b1308",
   "metadata": {},
   "source": [
    "## 2. Load and Explore the Dataset\n",
    "\n",
    "Now we'll load the legal dataset from a CSV file and explore its basic properties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8658ff88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "print(\"Loading the legal dataset...\")\n",
    "df = pd.read_csv('legal_dataset.csv')\n",
    "\n",
    "# Display basic information\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "display(df.head())\n",
    "\n",
    "print(\"\\nBasic information about the dataset:\")\n",
    "df.info()\n",
    "\n",
    "print(\"\\nSummary statistics:\")\n",
    "display(df.describe(include='all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fad05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"\\nChecking for missing values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Choose the text column to preprocess (first column)\n",
    "text_col = df.columns[0]\n",
    "print(f\"\\nWorking with text column: '{text_col}'\")\n",
    "\n",
    "# Display text length statistics\n",
    "df['text_length'] = df[text_col].astype(str).apply(len)\n",
    "df['word_count'] = df[text_col].astype(str).apply(lambda x: len(x.split()))\n",
    "df['sentence_count'] = df[text_col].astype(str).apply(lambda x: len(sent_tokenize(str(x))))\n",
    "\n",
    "print(\"\\nText statistics:\")\n",
    "print(f\"Average text length: {df['text_length'].mean():.2f} characters\")\n",
    "print(f\"Average word count: {df['word_count'].mean():.2f} words\")\n",
    "print(f\"Average sentence count: {df['sentence_count'].mean():.2f} sentences\")\n",
    "\n",
    "# Visualize text length distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(df['word_count'], kde=True)\n",
    "plt.title('Distribution of Word Count')\n",
    "plt.xlabel('Number of Words')\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(df['sentence_count'], kde=True)\n",
    "plt.title('Distribution of Sentence Count')\n",
    "plt.xlabel('Number of Sentences')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9236b6",
   "metadata": {},
   "source": [
    "## 3. Text Cleaning\n",
    "\n",
    "Before tokenization, we'll clean the text by:\n",
    "- Converting to lowercase\n",
    "- Removing special characters and digits\n",
    "- Removing extra whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3a5503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    # Convert to string in case there are non-string entries\n",
    "    text = str(text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Remove digits\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Apply cleaning\n",
    "df['cleaned_text'] = df[text_col].apply(clean_text)\n",
    "\n",
    "# Show cleaning example\n",
    "print(\"Text cleaning example:\")\n",
    "example_idx = 0  # Using the first document as an example\n",
    "original = df[text_col].iloc[example_idx]\n",
    "cleaned = df['cleaned_text'].iloc[example_idx]\n",
    "\n",
    "# Display the first 200 characters of each for comparison\n",
    "print(f\"\\nOriginal text (first 200 chars):\\n{original[:200]}...\")\n",
    "print(f\"\\nCleaned text (first 200 chars):\\n{cleaned[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f359a187",
   "metadata": {},
   "source": [
    "## 4. Tokenization\n",
    "\n",
    "Tokenization breaks text into smaller units such as sentences or words. We'll use NLTK's word_tokenize and sent_tokenize functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23e0e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word tokenization\n",
    "df['tokenized'] = df['cleaned_text'].apply(word_tokenize)\n",
    "\n",
    "# Sentence tokenization\n",
    "df['sentences'] = df[text_col].astype(str).apply(sent_tokenize)\n",
    "\n",
    "# Show tokenization example\n",
    "print(\"Tokenization example:\")\n",
    "example_idx = 0  # Using the first document as an example\n",
    "\n",
    "# Display first 15 tokens\n",
    "tokens = df['tokenized'].iloc[example_idx]\n",
    "print(f\"\\nFirst 15 word tokens: {tokens[:15]}\")\n",
    "\n",
    "# Display first 2 sentences (if available)\n",
    "sentences = df['sentences'].iloc[example_idx]\n",
    "if len(sentences) >= 2:\n",
    "    print(f\"\\nFirst 2 sentences:\")\n",
    "    for i, sent in enumerate(sentences[:2]):\n",
    "        print(f\"{i+1}. {sent}\")\n",
    "else:\n",
    "    print(f\"\\nSentences: {sentences}\")\n",
    "\n",
    "# Create a histogram of token counts\n",
    "token_counts = df['tokenized'].apply(len)\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(token_counts, kde=True, bins=30)\n",
    "plt.title('Distribution of Token Count per Document')\n",
    "plt.xlabel('Number of Tokens')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c314e9",
   "metadata": {},
   "source": [
    "## 5. Stopword Removal\n",
    "\n",
    "Stopwords are common words (like \"the\", \"a\", \"an\", \"in\") that don't carry much meaning in NLP analysis. We'll remove them using NLTK's stopwords list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac55574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(f\"Number of stopwords: {len(stop_words)}\")\n",
    "print(f\"Sample stopwords: {list(stop_words)[:10]}\")\n",
    "\n",
    "# Remove stopwords and non-alphanumeric tokens\n",
    "df['no_stopwords'] = df['tokenized'].apply(\n",
    "    lambda words: [w for w in words if w.lower() not in stop_words and w.isalnum()]\n",
    ")\n",
    "\n",
    "# Show stopwords removal example\n",
    "example_idx = 0  # Using the first document\n",
    "original_tokens = df['tokenized'].iloc[example_idx]\n",
    "filtered_tokens = df['no_stopwords'].iloc[example_idx]\n",
    "\n",
    "print(\"\\nStopword removal example:\")\n",
    "print(f\"Original token count: {len(original_tokens)}\")\n",
    "print(f\"After stopword removal: {len(filtered_tokens)}\")\n",
    "print(f\"\\nFirst 15 original tokens: {original_tokens[:15]}\")\n",
    "print(f\"First 15 filtered tokens: {filtered_tokens[:15]}\")\n",
    "\n",
    "# Calculate stopword percentage\n",
    "original_counts = df['tokenized'].apply(len)\n",
    "filtered_counts = df['no_stopwords'].apply(len)\n",
    "percent_removed = ((original_counts - filtered_counts) / original_counts * 100).mean()\n",
    "\n",
    "print(f\"\\nOn average, {percent_removed:.2f}% of tokens were removed as stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ac005a",
   "metadata": {},
   "source": [
    "## 6. Stemming\n",
    "\n",
    "Stemming reduces words to their root form (stem) by removing affixes. For example, \"running\", \"runs\", and \"ran\" all become \"run\". We'll use the Porter Stemmer algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5775e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Porter Stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Apply stemming to the filtered tokens\n",
    "df['stemmed'] = df['no_stopwords'].apply(lambda words: [stemmer.stem(w) for w in words])\n",
    "\n",
    "# Show stemming examples\n",
    "example_idx = 0\n",
    "filtered_tokens = df['no_stopwords'].iloc[example_idx]\n",
    "stemmed_tokens = df['stemmed'].iloc[example_idx]\n",
    "\n",
    "print(\"Stemming examples:\")\n",
    "print(f\"\\nFirst 15 tokens before stemming: {filtered_tokens[:15]}\")\n",
    "print(f\"First 15 tokens after stemming: {stemmed_tokens[:15]}\")\n",
    "\n",
    "# Display some interesting examples of stemming\n",
    "print(\"\\nSpecific stemming examples:\")\n",
    "examples = [\"running\", \"runs\", \"ran\", \"easily\", \"fairly\", \"wolves\", \"better\", \"was\", \"mice\"]\n",
    "for word in examples:\n",
    "    print(f\"{word} → {stemmer.stem(word)}\")\n",
    "\n",
    "# Show stemming results in a DataFrame for the first few tokens\n",
    "stem_comparison = pd.DataFrame({\n",
    "    'Original': filtered_tokens[:10],\n",
    "    'Stemmed': stemmed_tokens[:10]\n",
    "})\n",
    "display(stem_comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e40eed9",
   "metadata": {},
   "source": [
    "## 7. Lemmatization\n",
    "\n",
    "Lemmatization is similar to stemming but produces actual dictionary words. It's more sophisticated but slower than stemming. For example, \"better\" becomes \"good\" rather than \"better\" or \"bet\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b012d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize WordNet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Apply lemmatization to the filtered tokens\n",
    "df['lemmatized'] = df['no_stopwords'].apply(lambda words: [lemmatizer.lemmatize(w) for w in words])\n",
    "\n",
    "# Show lemmatization examples\n",
    "example_idx = 0\n",
    "filtered_tokens = df['no_stopwords'].iloc[example_idx]\n",
    "lemmatized_tokens = df['lemmatized'].iloc[example_idx]\n",
    "\n",
    "print(\"Lemmatization examples:\")\n",
    "print(f\"\\nFirst 15 tokens before lemmatization: {filtered_tokens[:15]}\")\n",
    "print(f\"First 15 tokens after lemmatization: {lemmatized_tokens[:15]}\")\n",
    "\n",
    "# Display some interesting examples of lemmatization\n",
    "print(\"\\nSpecific lemmatization examples:\")\n",
    "examples = [\"better\", \"worse\", \"running\", \"mice\", \"wolves\", \"are\", \"feet\", \"children\", \"companies\"]\n",
    "for word in examples:\n",
    "    print(f\"{word} → {lemmatizer.lemmatize(word)}\")\n",
    "\n",
    "# Compare stemming and lemmatization side by side\n",
    "comparison = []\n",
    "for word in examples:\n",
    "    comparison.append({\n",
    "        'Original': word,\n",
    "        'Stemmed': stemmer.stem(word),\n",
    "        'Lemmatized': lemmatizer.lemmatize(word)\n",
    "    })\n",
    "    \n",
    "comparison_df = pd.DataFrame(comparison)\n",
    "display(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7e1c4f",
   "metadata": {},
   "source": [
    "## 8. Part-of-Speech Tagging\n",
    "\n",
    "POS tagging identifies the grammatical parts of speech of each word (noun, verb, adjective, etc.). This can be helpful for lemmatization and for understanding the structure of sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35318dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply POS tagging to the filtered tokens\n",
    "df['pos_tags'] = df['no_stopwords'].apply(pos_tag)\n",
    "\n",
    "# Show POS tagging examples\n",
    "example_idx = 0\n",
    "pos_tagged = df['pos_tags'].iloc[example_idx]\n",
    "\n",
    "print(\"Part-of-Speech tagging example:\")\n",
    "print(\"\\nFirst 15 tokens with POS tags:\")\n",
    "for i, (word, tag) in enumerate(pos_tagged[:15]):\n",
    "    print(f\"{i+1}. {word} → {tag}\")\n",
    "\n",
    "# Explain common POS tags\n",
    "pos_tag_explanations = {\n",
    "    'CC': 'Coordinating conjunction',\n",
    "    'CD': 'Cardinal number',\n",
    "    'DT': 'Determiner',\n",
    "    'IN': 'Preposition or subordinating conjunction',\n",
    "    'JJ': 'Adjective',\n",
    "    'JJR': 'Adjective, comparative',\n",
    "    'JJS': 'Adjective, superlative',\n",
    "    'NN': 'Noun, singular or mass',\n",
    "    'NNP': 'Proper noun, singular',\n",
    "    'NNPS': 'Proper noun, plural',\n",
    "    'NNS': 'Noun, plural',\n",
    "    'RB': 'Adverb',\n",
    "    'VB': 'Verb, base form',\n",
    "    'VBD': 'Verb, past tense',\n",
    "    'VBG': 'Verb, gerund or present participle',\n",
    "    'VBN': 'Verb, past participle',\n",
    "    'VBP': 'Verb, non-3rd person singular present',\n",
    "    'VBZ': 'Verb, 3rd person singular present'\n",
    "}\n",
    "\n",
    "print(\"\\nCommon POS tag explanations:\")\n",
    "for tag, explanation in list(pos_tag_explanations.items())[:10]:\n",
    "    print(f\"{tag}: {explanation}\")\n",
    "\n",
    "# Count frequency of each POS tag\n",
    "all_tags = [tag for doc_tags in df['pos_tags'] for _, tag in doc_tags]\n",
    "tag_freq = Counter(all_tags)\n",
    "\n",
    "# Display the most common POS tags\n",
    "print(\"\\nMost common POS tags in the dataset:\")\n",
    "for tag, count in tag_freq.most_common(10):\n",
    "    explanation = pos_tag_explanations.get(tag, \"Other\")\n",
    "    print(f\"{tag} ({explanation}): {count} occurrences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f568c408",
   "metadata": {},
   "source": [
    "## 9. Text Visualization\n",
    "\n",
    "Visualizing text data can provide insights about the most frequent terms and their relationships. Let's create a word cloud and frequency distribution plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7de50a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all words from the lemmatized tokens\n",
    "all_lemmas = [word for doc_lemmas in df['lemmatized'] for word in doc_lemmas]\n",
    "lemma_freq = Counter(all_lemmas)\n",
    "\n",
    "# Plot frequency distribution of top words\n",
    "top_n = 20\n",
    "top_words = dict(lemma_freq.most_common(top_n))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(top_words.keys(), top_words.values())\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title(f'Top {top_n} Most Frequent Words After Preprocessing')\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Generate a word cloud\n",
    "try:\n",
    "    # Create a text string for the word cloud\n",
    "    lemmatized_text = ' '.join(all_lemmas)\n",
    "    \n",
    "    # Generate word cloud\n",
    "    wordcloud = WordCloud(width=800, height=400, \n",
    "                         background_color='white',\n",
    "                         max_words=200,\n",
    "                         contour_width=3,\n",
    "                         contour_color='steelblue')\n",
    "    \n",
    "    wordcloud.generate(lemmatized_text)\n",
    "    \n",
    "    # Display the word cloud\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title('Word Cloud of Lemmatized Words')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Couldn't generate word cloud: {e}\")\n",
    "    print(\"You may need to install the wordcloud package: pip install wordcloud\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd7d98b",
   "metadata": {},
   "source": [
    "## 10. Vectorization\n",
    "\n",
    "To use our preprocessed text with machine learning algorithms, we need to convert the text into numerical vectors. We'll demonstrate two common approaches:\n",
    "\n",
    "1. Bag of Words (Count Vectorization)\n",
    "2. TF-IDF (Term Frequency-Inverse Document Frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43872b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare text for vectorization by joining tokens\n",
    "df['processed_text'] = df['lemmatized'].apply(lambda tokens: ' '.join(tokens))\n",
    "\n",
    "# Create a smaller sample for demonstration if needed\n",
    "sample_texts = df['processed_text'].tolist()\n",
    "\n",
    "print(f\"Number of documents to vectorize: {len(sample_texts)}\")\n",
    "print(f\"Sample document: {sample_texts[0][:100]}...\")\n",
    "\n",
    "# 1. Bag of Words Vectorization\n",
    "count_vectorizer = CountVectorizer(max_features=1000)  # Limit to top 1000 features\n",
    "bow_matrix = count_vectorizer.fit_transform(sample_texts)\n",
    "\n",
    "print(\"\\nBag of Words Vectorization:\")\n",
    "print(f\"Shape of BoW matrix: {bow_matrix.shape}\")\n",
    "print(f\"Number of unique words in vocabulary: {len(count_vectorizer.vocabulary_)}\")\n",
    "print(f\"First 10 features: {list(count_vectorizer.vocabulary_.keys())[:10]}\")\n",
    "\n",
    "# 2. TF-IDF Vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(sample_texts)\n",
    "\n",
    "print(\"\\nTF-IDF Vectorization:\")\n",
    "print(f\"Shape of TF-IDF matrix: {tfidf_matrix.shape}\")\n",
    "print(f\"Number of unique words in vocabulary: {len(tfidf_vectorizer.vocabulary_)}\")\n",
    "\n",
    "# Display a sample document as vectors\n",
    "doc_idx = 0  # First document\n",
    "\n",
    "# Get the non-zero features for this document\n",
    "bow_features = bow_matrix[doc_idx].nonzero()[1]\n",
    "tfidf_features = tfidf_matrix[doc_idx].nonzero()[1]\n",
    "\n",
    "# Display as a table\n",
    "print(\"\\nSample of vectorization for the first document:\")\n",
    "print(\"\\nBag of Words representation (first 10 non-zero features):\")\n",
    "for i in bow_features[:10]:\n",
    "    feature_name = count_vectorizer.get_feature_names_out()[i]\n",
    "    feature_value = bow_matrix[doc_idx, i]\n",
    "    print(f\"{feature_name}: {feature_value}\")\n",
    "\n",
    "print(\"\\nTF-IDF representation (first 10 non-zero features):\")\n",
    "for i in tfidf_features[:10]:\n",
    "    feature_name = tfidf_vectorizer.get_feature_names_out()[i]\n",
    "    feature_value = tfidf_matrix[doc_idx, i]\n",
    "    print(f\"{feature_name}: {feature_value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb150d94",
   "metadata": {},
   "source": [
    "## 11. Save Processed Data\n",
    "\n",
    "Finally, let's save our preprocessed data for future use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d257f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the processed DataFrame to CSV\n",
    "df.to_csv('legal_dataset_preprocessed.csv', index=False)\n",
    "print(\"Saved preprocessed data to legal_dataset_preprocessed.csv\")\n",
    "\n",
    "# Save vectorized data (optional)\n",
    "# Convert sparse matrices to DataFrames\n",
    "bow_df = pd.DataFrame(bow_matrix.toarray(), columns=count_vectorizer.get_feature_names_out())\n",
    "bow_df.to_csv('legal_dataset_bow_vectors.csv', index=False)\n",
    "print(\"Saved Bag of Words vectors to legal_dataset_bow_vectors.csv\")\n",
    "\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "tfidf_df.to_csv('legal_dataset_tfidf_vectors.csv', index=False)\n",
    "print(\"Saved TF-IDF vectors to legal_dataset_tfidf_vectors.csv\")\n",
    "\n",
    "# Display final dataset columns\n",
    "print(\"\\nFinal preprocessed DataFrame columns:\")\n",
    "for i, col in enumerate(df.columns):\n",
    "    print(f\"{i+1}. {col}\")\n",
    "\n",
    "print(\"\\nPreprocessing pipeline complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40ad99c",
   "metadata": {},
   "source": [
    "## 12. Conclusion\n",
    "\n",
    "In this notebook, we've covered the essential steps in NLP preprocessing:\n",
    "\n",
    "1. **Data Exploration**: Understanding the structure and content of our dataset\n",
    "2. **Text Cleaning**: Removing special characters and normalizing text\n",
    "3. **Tokenization**: Breaking text into individual words and sentences\n",
    "4. **Stopword Removal**: Filtering out common words that don't carry much meaning\n",
    "5. **Stemming**: Reducing words to their word stem\n",
    "6. **Lemmatization**: Converting words to their base dictionary form\n",
    "7. **Part-of-Speech Tagging**: Identifying grammatical components\n",
    "8. **Visualization**: Creating word clouds and frequency plots\n",
    "9. **Vectorization**: Converting text to numerical vectors for machine learning\n",
    "\n",
    "These preprocessing steps are fundamental for any NLP task, including:\n",
    "- Text classification\n",
    "- Sentiment analysis\n",
    "- Document clustering\n",
    "- Topic modeling\n",
    "- Information retrieval\n",
    "- And more!\n",
    "\n",
    "With our processed data, we're now ready to apply various machine learning algorithms for deeper analysis."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
