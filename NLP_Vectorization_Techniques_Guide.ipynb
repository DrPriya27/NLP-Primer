{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eedf755a",
   "metadata": {},
   "source": [
    "# NLP Vectorization Techniques: A Practical Guide\n",
    "\n",
    "This notebook demonstrates various NLP vectorization techniques inspired by the Neptune.ai guide: [Vectorization Techniques in NLP](https://neptune.ai/blog/vectorization-techniques-in-nlp-guide).\n",
    "\n",
    "**Outline:**\n",
    "1. Import Required Libraries\n",
    "2. Load and Explore Sample Text Data\n",
    "3. Bag of Words Vectorization\n",
    "4. TF-IDF Vectorization\n",
    "5. N-gram Vectorization\n",
    "6. Word2Vec Embeddings\n",
    "7. GloVe Embeddings\n",
    "8. FastText Embeddings\n",
    "9. Visualize Word Embeddings\n",
    "10. Finding Similar Words and Analogies\n",
    "11. Document Embeddings (Averaging, Doc2Vec)\n",
    "12. Using Pretrained Embeddings in ML Pipelines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5206bd3",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "We will use pandas, numpy, scikit-learn, gensim, and matplotlib for NLP vectorization and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "431eb9b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m classification_report\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Word2Vec, FastText\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdoc2vec\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Doc2Vec, TaggedDocument\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "# Import Required Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import gensim\n",
    "from gensim.models import Word2Vec, FastText\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import nltk\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda223ad",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Sample Text Data\n",
    "\n",
    "Let's create a small sample corpus for demonstration and display its contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca07b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample corpus for demonstration\n",
    "corpus = [\n",
    "    'The quick brown fox jumps over the lazy dog',\n",
    "    'Never jump over the lazy dog quickly',\n",
    "    'A fox is quick and brown',\n",
    "    'Dogs are loyal and friendly animals',\n",
    "    'Foxes are wild animals and very clever',\n",
    "    'The dog barked at the fox',\n",
    "    'Quick brown foxes leap over lazy dogs in summer',\n",
    "    'A lazy dog sleeps all day',\n",
    "    'Wild animals live in the forest',\n",
    "    'Friendly dogs make great pets'\n",
    "]\n",
    "\n",
    "# Display the corpus\n",
    "df_corpus = pd.DataFrame({'Text': corpus})\n",
    "df_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281037cf",
   "metadata": {},
   "source": [
    "## 3. Bag of Words Vectorization\n",
    "\n",
    "Bag of Words (BoW) is a simple and commonly used method to convert text into numerical vectors by counting word occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e983781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of Words Vectorization\n",
    "vectorizer_bow = CountVectorizer()\n",
    "X_bow = vectorizer_bow.fit_transform(corpus)\n",
    "X.toarray()\n",
    "df_bow = pd.DataFrame(X_bow.toarray(), columns=vectorizer_bow.get_feature_names_out())\n",
    "df_bow\n",
    "\n",
    "Let’s print the vocabulary to understand why it looks like this.\n",
    "\n",
    "sorted(cv.vocabulary_.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d546cf21",
   "metadata": {},
   "source": [
    "## 4. TF-IDF Vectorization\n",
    "\n",
    "TF-IDF (Term Frequency–Inverse Document Frequency) weighs the frequency of a word in a document against its frequency in the entire corpus, reducing the impact of common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac970ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Vectorization\n",
    "vectorizer_tfidf = TfidfVectorizer()\n",
    "X_tfidf = vectorizer_tfidf.fit_transform(corpus)\n",
    "df_tfidf = pd.DataFrame(X_tfidf.toarray(), columns=vectorizer_tfidf.get_feature_names_out())\n",
    "df_tfidf.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a633e2b",
   "metadata": {},
   "source": [
    "## 5. N-gram Vectorization\n",
    "\n",
    "N-grams are contiguous sequences of n items (words) from a given text. Using n-grams (bigrams, trigrams, etc.) can capture more context than single words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a30e55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb8faf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-gram (Bigram) Vectorization\n",
    "vectorizer_bigram = CountVectorizer(ngram_range=(2,2))\n",
    "X_bigram = vectorizer_bigram.fit_transform(corpus)\n",
    "df_bigram = pd.DataFrame(X_bigram.toarray(), columns=vectorizer_bigram.get_feature_names_out())\n",
    "df_bigram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d2e425",
   "metadata": {},
   "source": [
    "## 6. Word2Vec Embeddings\n",
    "\n",
    "Word2Vec is a neural network-based technique that learns dense vector representations for words, capturing semantic relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f806ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize corpus for Word2Vec\n",
    "corpus_tokenized = [nltk.word_tokenize(doc.lower()) for doc in corpus]\n",
    "\n",
    "# Train Word2Vec model\n",
    "w2v_model = Word2Vec(sentences=corpus_tokenized, vector_size=50, window=3, min_count=1, workers=1, seed=42)\n",
    "\n",
    "# Get vector for a word\n",
    "print('Vector for \"fox\":', w2v_model.wv['fox'])\n",
    "\n",
    "# Find most similar words to 'fox'\n",
    "print('Most similar to \"fox\":', w2v_model.wv.most_similar('fox'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525296da",
   "metadata": {},
   "source": [
    "## 7. GloVe Embeddings\n",
    "\n",
    "GloVe (Global Vectors for Word Representation) is a popular pretrained word embedding method that captures global word-word co-occurrence statistics from a corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814bd6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and load GloVe embeddings (50d for demo)\n",
    "glove_path = 'glove.6B.50d.txt'\n",
    "if not os.path.exists(glove_path):\n",
    "    url = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
    "    r = requests.get(url)\n",
    "    z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "    z.extract('glove.6B.50d.txt')\n",
    "\n",
    "glove_vectors = {}\n",
    "with open(glove_path, encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype='float32')\n",
    "        glove_vectors[word] = vector\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "vec_fox = glove_vectors.get('fox')\n",
    "vec_dog = glove_vectors.get('dog')\n",
    "if vec_fox is not None and vec_dog is not None:\n",
    "    print('Cosine similarity between fox and dog:', cosine_similarity(vec_fox, vec_dog))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a2f1a9",
   "metadata": {},
   "source": [
    "## 8. FastText Embeddings\n",
    "\n",
    "FastText, developed by Facebook, extends Word2Vec by representing words as bags of character n-grams, allowing it to generate embeddings for out-of-vocabulary words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5a2a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train FastText model\n",
    "ft_model = FastText(sentences=corpus_tokenized, vector_size=50, window=3, min_count=1, workers=1, seed=42)\n",
    "\n",
    "# Get vector for a word\n",
    "print('Vector for \"fox\":', ft_model.wv['fox'])\n",
    "\n",
    "# Get vector for an out-of-vocabulary word (e.g., 'foxes')\n",
    "print('Vector for \"foxes\":', ft_model.wv['foxes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e221553",
   "metadata": {},
   "source": [
    "## 9. Visualize Word Embeddings\n",
    "\n",
    "We can use dimensionality reduction techniques like t-SNE or PCA to visualize high-dimensional word embeddings in 2D space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1327c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Word2Vec Embeddings with t-SNE\n",
    "words = list(w2v_model.wv.index_to_key)\n",
    "word_vectors = np.array([w2v_model.wv[w] for w in words])\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=5)\n",
    "word_vec_2d = tsne.fit_transform(word_vectors)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(word_vec_2d[:,0], word_vec_2d[:,1])\n",
    "for i, word in enumerate(words):\n",
    "    plt.annotate(word, xy=(word_vec_2d[i,0], word_vec_2d[i,1]))\n",
    "plt.title('Word2Vec Embeddings Visualized with t-SNE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f66d07",
   "metadata": {},
   "source": [
    "## 10. Finding Similar Words and Analogies\n",
    "\n",
    "Embedding models can be used to find similar words and solve analogy tasks (e.g., king - man + woman ≈ queen)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc1e40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find most similar words to 'dog'\n",
    "print('Most similar to \"dog\":', w2v_model.wv.most_similar('dog'))\n",
    "\n",
    "# Analogy: 'fox' - 'dog' + 'cat' ≈ ?\n",
    "try:\n",
    "    result = w2v_model.wv.most_similar(positive=['fox', 'cat'], negative=['dog'])\n",
    "    print(\"'fox' - 'dog' + 'cat' ≈\", result[0][0])\n",
    "except Exception as e:\n",
    "    print('Analogy not found:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d2a7d6",
   "metadata": {},
   "source": [
    "## 11. Document Embeddings (Averaging, Doc2Vec)\n",
    "\n",
    "We can represent entire documents by averaging word vectors or using Doc2Vec for document-level embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675e982f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document Embeddings by Averaging Word2Vec Vectors\n",
    "def document_vector(doc):\n",
    "    words = [w for w in nltk.word_tokenize(doc.lower()) if w in w2v_model.wv]\n",
    "    if words:\n",
    "        return np.mean([w2v_model.wv[w] for w in words], axis=0)\n",
    "    else:\n",
    "        return np.zeros(w2v_model.vector_size)\n",
    "\n",
    "doc_vectors = np.array([document_vector(doc) for doc in corpus])\n",
    "print('Shape of document vectors (averaged):', doc_vectors.shape)\n",
    "\n",
    "# Doc2Vec Example\n",
    "tagged_data = [TaggedDocument(words=nltk.word_tokenize(doc.lower()), tags=[str(i)]) for i, doc in enumerate(corpus)]\n",
    "d2v_model = Doc2Vec(vector_size=50, window=2, min_count=1, workers=1, epochs=40, seed=42)\n",
    "d2v_model.build_vocab(tagged_data)\n",
    "d2v_model.train(tagged_data, total_examples=d2v_model.corpus_count, epochs=d2v_model.epochs)\n",
    "\n",
    "# Get vector for first document\n",
    "print('Doc2Vec vector for first document:', d2v_model.dv['0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77773355",
   "metadata": {},
   "source": [
    "## 12. Using Pretrained Embeddings in ML Pipelines\n",
    "\n",
    "We can integrate pretrained embeddings into scikit-learn pipelines for text classification tasks. Here, we'll use averaged Word2Vec vectors as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494226e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Text Classification with Averaged Word2Vec Vectors\n",
    "# Create dummy labels for demonstration (e.g., 0 for animal-related, 1 for pet-related)\n",
    "labels = [0,0,0,1,0,1,0,1,0,1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(doc_vectors, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "clf = LogisticRegression(max_iter=200)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
