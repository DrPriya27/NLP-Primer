{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17974cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upgrade pip, setuptools, wheel and install gensim\n",
    "%pip install --upgrade pip setuptools wheel\n",
    "%pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b68658",
   "metadata": {},
   "source": [
    "# Day 3: Text representation - Word Vectorization & Embeddings\n",
    "\n",
    "## What is vectorization? \n",
    "Vectorization is a process of converting input text data into vectors of real numbers which is the format that ML models support. \n",
    "\n",
    "This notebook covers advanced NLP representation techniques, focusing on word vectorization, word embeddings, and their applications. You'll learn how to move beyond Bag-of-Words and TF-IDF to dense vector representations, visualize embeddings, and use them in downstream tasks.\n",
    "\n",
    "**Outline:**\n",
    "- Import Required Libraries\n",
    "- Load and Explore the Dataset\n",
    "- Text Preprocessing Recap\n",
    "- Word Embeddings: Introduction\n",
    "- Word2Vec Embedding with Gensim\n",
    "- GloVe Embedding Integration\n",
    "- Visualizing Word Embeddings\n",
    "- Finding Similar Words and Analogies\n",
    "- Document Embeddings (Averaging, Doc2Vec)\n",
    "- Using Pretrained Embeddings in Scikit-learn Pipelines\n",
    "- Save Embeddings and Processed Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb99f8e6",
   "metadata": {},
   "source": [
    "## 1. Bag of Words\n",
    "\n",
    "It involves three major steps [Tokenization], Vocabulary creation and Vector creation (via considering frequency of vocabulary words in a given document).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7eeee120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 0 0 1 1 0 1 1 0 2]\n",
      " [0 0 1 0 0 1 0 1 1 1 0 1 1]\n",
      " [1 1 0 1 1 0 0 0 0 0 1 0 0]]\n",
      "['and', 'brown', 'dog', 'fox', 'is', 'jump', 'jumps', 'lazy', 'never', 'over', 'quick', 'quickly', 'the']\n",
      "   and  brown  dog  fox  is  jump  jumps  lazy  never  over  quick  quickly  \\\n",
      "0    0      1    1    1   0     0      1     1      0     1      1        0   \n",
      "1    0      0    1    0   0     1      0     1      1     1      0        1   \n",
      "2    1      1    0    1   1     0      0     0      0     0      1        0   \n",
      "\n",
      "   the  \n",
      "0    2  \n",
      "1    1  \n",
      "2    0  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>brown</th>\n",
       "      <th>dog</th>\n",
       "      <th>fox</th>\n",
       "      <th>is</th>\n",
       "      <th>jump</th>\n",
       "      <th>jumps</th>\n",
       "      <th>lazy</th>\n",
       "      <th>never</th>\n",
       "      <th>over</th>\n",
       "      <th>quick</th>\n",
       "      <th>quickly</th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   and  brown  dog  fox  is  jump  jumps  lazy  never  over  quick  quickly  \\\n",
       "0    0      1    1    1   0     0      1     1      0     1      1        0   \n",
       "1    0      0    1    0   0     1      0     1      1     1      0        1   \n",
       "2    1      1    0    1   1     0      0     0      0     0      1        0   \n",
       "\n",
       "   the  \n",
       "0    2  \n",
       "1    1  \n",
       "2    0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bag of Words Example\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "corpus = [\n",
    "    'The quick brown fox jumps over the lazy dog',\n",
    "    'Never jump over the lazy dog quickly',\n",
    "    'A fox is quick and brown'\n",
    " ]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(X.toarray())\n",
    "#Let’s print the vocabulary to understand why it looks like this.\n",
    "print(sorted(vectorizer.vocabulary_.keys()))\n",
    "\n",
    "# The X returned by fit_transform is a sparse matrix for memory efficiency.\n",
    "# .toarray() converts this sparse matrix to a dense NumPy array, so we can easily create a DataFrame.\n",
    "df_bow = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "print(df_bow)\n",
    "df_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff25560",
   "metadata": {},
   "source": [
    "## 2. Term Frequency–Inverse Document Frequency (TF-IDF)\n",
    "\n",
    "TF stands for Term Frequency. It can be understood as a normalized frequency score. IDF is a reciprocal of the Document Frequency. Please refer to [article](https://neptune.ai/blog/vectorization-techniques-in-nlp-guide) for more details.\n",
    "\n",
    "\n",
    "Traditional vectorization methods like Bag-of-Words and TF-IDF create high-dimensional, sparse vectors and do not capture word meaning or context. Word embeddings (Word2Vec, GloVe, FastText) create dense, low-dimensional vectors that capture semantic relationships between words.\n",
    "\n",
    "TF-IDF (Term Frequency–Inverse Document Frequency) weighs the frequency of a word in a document against its frequency in the entire corpus, reducing the impact of common words.\n",
    "\n",
    "TF-IDF or Term Frequency–Inverse Document Frequency, is a numerical statistic that’s intended to reflect how important a word is to a document. Although it’s another frequency-based method, it’s not as naive as Bag of Words.\n",
    "\n",
    "How does TF-IDF improve over Bag of Words?\n",
    "\n",
    "In Bag of Words, we witnessed how vectorization was just concerned with the frequency of vocabulary words in a given document. As a result, articles, prepositions, and conjunctions which don’t contribute a lot to the meaning get as much importance as, say, adjectives. \n",
    "\n",
    "TF-IDF helps us to overcome this issue. Words that get repeated too often don’t overpower less frequent but important words.\n",
    "\n",
    "It has two parts:\n",
    "\n",
    "TF\n",
    "TF stands for Term Frequency. It can be understood as a normalized frequency score. It is calculated via the following formula:\n",
    "\n",
    "\n",
    "So one can imagine that this number will always stay ≤ 1, thus we now judge how frequent a word is in the context of all of the words in a document.\n",
    "\n",
    "IDF\n",
    "IDF stands for Inverse Document Frequency, but before we go into IDF, we must make sense of DF – Document Frequency. It’s given by the following formula:\n",
    "\n",
    "\n",
    "DF tells us about the proportion of documents that contain a certain word. So what’s IDF?\n",
    "\n",
    "It’s the reciprocal of the Document Frequency, and the final IDF score comes out of the following formula:\n",
    "\n",
    "\n",
    "Why inverse the DF?\n",
    "\n",
    "Just as we discussed above, the intuition behind it is that the more common a word is across all documents, the lesser its importance is for the current document.\n",
    "\n",
    "A logarithm is taken to dampen the effect of IDF in the final calculation.\n",
    "\n",
    "The final TF-IDF score comes out to be:\n",
    "\n",
    "\n",
    "This is how TF-IDF manages to incorporate the significance of a word. The higher the score, the more important that word is.\n",
    "\n",
    "Let’s get our hands dirty now and see how TF-IDF looks in practice.\n",
    "\n",
    "Again, we’ll be using the Sklearn library for this exercise, just as we did in the case of Bag of Words.\n",
    "\n",
    "Making the required imports.\n",
    "\n",
    "https://medium.com/analytics-vidhya/understanding-tf-idf-in-nlp-4a28eebdee6a\n",
    "\n",
    "https://neptune.ai/blog/vectorization-techniques-in-nlp-guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4e18f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    and  brown   dog   fox    is  jump  jumps  lazy  never  over  quick  \\\n",
      "0  0.00   0.29  0.29  0.29  0.00  0.00   0.38  0.29   0.00  0.29   0.29   \n",
      "1  0.00   0.00  0.33  0.00  0.00  0.43   0.00  0.33   0.43  0.33   0.00   \n",
      "2  0.52   0.39  0.00  0.39  0.52  0.00   0.00  0.00   0.00  0.00   0.39   \n",
      "\n",
      "   quickly   the  \n",
      "0     0.00  0.58  \n",
      "1     0.43  0.33  \n",
      "2     0.00  0.00  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>brown</th>\n",
       "      <th>dog</th>\n",
       "      <th>fox</th>\n",
       "      <th>is</th>\n",
       "      <th>jump</th>\n",
       "      <th>jumps</th>\n",
       "      <th>lazy</th>\n",
       "      <th>never</th>\n",
       "      <th>over</th>\n",
       "      <th>quick</th>\n",
       "      <th>quickly</th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.291992</td>\n",
       "      <td>0.291992</td>\n",
       "      <td>0.291992</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.383935</td>\n",
       "      <td>0.291992</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.291992</td>\n",
       "      <td>0.291992</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.583984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.329928</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.433816</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.329928</td>\n",
       "      <td>0.433816</td>\n",
       "      <td>0.329928</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.433816</td>\n",
       "      <td>0.329928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.51742</td>\n",
       "      <td>0.393511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.393511</td>\n",
       "      <td>0.51742</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.393511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       and     brown       dog       fox       is      jump     jumps  \\\n",
       "0  0.00000  0.291992  0.291992  0.291992  0.00000  0.000000  0.383935   \n",
       "1  0.00000  0.000000  0.329928  0.000000  0.00000  0.433816  0.000000   \n",
       "2  0.51742  0.393511  0.000000  0.393511  0.51742  0.000000  0.000000   \n",
       "\n",
       "       lazy     never      over     quick   quickly       the  \n",
       "0  0.291992  0.000000  0.291992  0.291992  0.000000  0.583984  \n",
       "1  0.329928  0.433816  0.329928  0.000000  0.433816  0.329928  \n",
       "2  0.000000  0.000000  0.000000  0.393511  0.000000  0.000000  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF Example\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\n",
    "    'The quick brown fox jumps over the lazy dog',\n",
    "    'Never jump over the lazy dog quickly',\n",
    "    'A fox is quick and brown'\n",
    " ]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "df_tfidf = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "print(df_tfidf.round(2))\n",
    "df_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41e0426",
   "metadata": {},
   "source": [
    "## 5. N-gram Vectorization\n",
    "\n",
    "N-grams are contiguous sequences of n items (words) from a given text. Using n-grams (bigrams, trigrams, etc.) can capture more context than single words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f66ed330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and brown</th>\n",
       "      <th>brown fox</th>\n",
       "      <th>dog quickly</th>\n",
       "      <th>fox is</th>\n",
       "      <th>fox jumps</th>\n",
       "      <th>is quick</th>\n",
       "      <th>jump over</th>\n",
       "      <th>jumps over</th>\n",
       "      <th>lazy dog</th>\n",
       "      <th>never jump</th>\n",
       "      <th>over the</th>\n",
       "      <th>quick and</th>\n",
       "      <th>quick brown</th>\n",
       "      <th>the lazy</th>\n",
       "      <th>the quick</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   and brown  brown fox  dog quickly  fox is  fox jumps  is quick  jump over  \\\n",
       "0          0          1            0       0          1         0          0   \n",
       "1          0          0            1       0          0         0          1   \n",
       "2          1          0            0       1          0         1          0   \n",
       "\n",
       "   jumps over  lazy dog  never jump  over the  quick and  quick brown  \\\n",
       "0           1         1           0         1          0            1   \n",
       "1           0         1           1         1          0            0   \n",
       "2           0         0           0         0          1            0   \n",
       "\n",
       "   the lazy  the quick  \n",
       "0         1          1  \n",
       "1         1          0  \n",
       "2         0          0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# N-gram (Bigram) Vectorization\n",
    "vectorizer_bigram = CountVectorizer(ngram_range=(2,2))\n",
    "X_bigram = vectorizer_bigram.fit_transform(corpus)\n",
    "df_bigram = pd.DataFrame(X_bigram.toarray(), columns=vectorizer_bigram.get_feature_names_out())\n",
    "df_bigram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcf8cac",
   "metadata": {},
   "source": [
    "## 3. [Word2Vec](https://arxiv.org/pdf/1301.3781.pdf)\n",
    "\n",
    "Neural Network based method to generate [word embeddings](https://neptune.ai/blog/word-embeddings-guide). In earlier two methods, semantics were completely ignored. With the introduction of Word2Vec, the vector representation of words was said to be contextually aware, probably for the first time ever.\n",
    "\n",
    "## 6. Word2Vec Embeddings\n",
    "\n",
    "Word2Vec is a neural network-based technique that learns dense vector representations for words, capturing semantic relationships.\n",
    "\n",
    "Traditional vectorization methods like Bag-of-Words and TF-IDF create high-dimensional, sparse vectors and do not capture word meaning or context. Word embeddings (Word2Vec, GloVe, FastText) create dense, low-dimensional vectors that capture semantic relationships between words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9e6db89",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Word2Vec Example with Gensim\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Word2Vec\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\n\u001b[32m      4\u001b[39m nltk.download(\u001b[33m'\u001b[39m\u001b[33mpunkt_tab\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "# Word2Vec Example with Gensim\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "corpus = [\n",
    "    'The quick brown fox jumps over the lazy dog',\n",
    "    'Never jump over the lazy dog quickly',\n",
    "    'A fox is quick and brown'\n",
    " ]\n",
    "\n",
    "# Tokenize sentences\n",
    "tokenized_corpus = [nltk.word_tokenize(sentence.lower()) for sentence in corpus]\n",
    "\n",
    "# Train Word2Vec model\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=tokenized_corpus,  # List of tokenized sentences (list of list of words)\n",
    "    vector_size=50,              # Dimensionality of the word vectors (embedding size)\n",
    "    window=3,                    # Maximum distance between the current and predicted word within a sentence\n",
    "    min_count=1,                 # Ignores all words with total frequency lower than this\n",
    "    workers=1,                   # Number of CPU cores to use during training\n",
    "    seed=42                      # Random seed for reproducibility\n",
    ")\n",
    "\n",
    "# Get vector for a word\n",
    "print('Vector for \"fox\":', w2v_model.wv['fox'])\n",
    "\n",
    "# Find most similar words\n",
    "print('Most similar to \"fox\":', w2v_model.wv.most_similar('fox'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1231aa6",
   "metadata": {},
   "source": [
    "### 4. Global Vectors for word representation [(GloVe)](https://nlp.stanford.edu/pubs/glove.pdf)\n",
    "It is also based on creating contextual word embeddings. Word2Vec is a window-based method, in which the model relies on local information for generating word embeddings, which in turn is limited to the window size that we choose. GloVe on the other hand captures both global and local statistics in order to come up with the word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4231eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GloVe Example: Using Pretrained Embeddings\n",
    "import numpy as np\n",
    "import requests, zipfile, io, os\n",
    "\n",
    "# Download GloVe embeddings (small sample for demo)\n",
    "if not os.path.exists('glove.6B.50d.txt'):\n",
    "    url = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
    "    r = requests.get(url)\n",
    "    z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "    z.extract('glove.6B.50d.txt')\n",
    "\n",
    "# Load GloVe vectors\n",
    "glove_vectors = {}\n",
    "with open('glove.6B.50d.txt', encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype='float32')\n",
    "        glove_vectors[word] = vector\n",
    "\n",
    "# Example: Get vector for 'fox' and find cosine similarity with 'dog'\n",
    "from numpy.linalg import norm\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (norm(vec1) * norm(vec2))\n",
    "\n",
    "vec_fox = glove_vectors['fox']\n",
    "vec_dog = glove_vectors['dog']\n",
    "print('Cosine similarity between fox and dog:', cosine_similarity(vec_fox, vec_dog))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6948c3",
   "metadata": {},
   "source": [
    "### 6. Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b97e969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doc2Vec Example with Gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "corpus = [\n",
    "    'The quick brown fox jumps over the lazy dog',\n",
    "    'Never jump over the lazy dog quickly',\n",
    "    'A fox is quick and brown'\n",
    " ]\n",
    "\n",
    "# Tag documents\n",
    "tagged_data = [TaggedDocument(words=nltk.word_tokenize(doc.lower()), tags=[str(i)]) for i, doc in enumerate(corpus)]\n",
    "\n",
    "# Train Doc2Vec model\n",
    "doc2vec_model = Doc2Vec(vector_size=50, window=2, min_count=1, workers=1, epochs=40, seed=42)\n",
    "doc2vec_model.build_vocab(tagged_data)\n",
    "doc2vec_model.train(tagged_data, total_examples=doc2vec_model.corpus_count, epochs=doc2vec_model.epochs)\n",
    "\n",
    "# Get vector for first document\n",
    "print('Vector for first document:', doc2vec_model.dv['0'])\n",
    "\n",
    "# Find most similar documents\n",
    "print('Most similar to first document:', doc2vec_model.dv.most_similar('0'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
