{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef27e4de",
   "metadata": {},
   "source": [
    "# NLP Preprocessing with NLTK\n",
    "\n",
    "This notebook demonstrates various Natural Language Processing (NLP) preprocessing techniques using the Natural Language Toolkit (NLTK) library. We'll cover essential preprocessing steps that are commonly used in NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da32eb10",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "First, we need to install NLTK and download the required data packages. If you haven't installed NLTK yet, you can install it using pip:\n",
    "```bash\n",
    "pip install nltk\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0d9214",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Download required NLTK data packages\n",
    "nltk.download('punkt')        # For tokenization\n",
    "nltk.download('stopwords')    # For stopword removal\n",
    "nltk.download('wordnet')      # For lemmatization\n",
    "nltk.download('averaged_perceptron_tagger')  # For POS tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f508fda0",
   "metadata": {},
   "source": [
    "## 2. Text Preprocessing Functions\n",
    "\n",
    "Let's import the necessary NLTK modules and create some utility functions for text preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b6ea47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Sample text for demonstration\n",
    "sample_text = \"\"\"Natural Language Processing (NLP) is a fascinating field of artificial intelligence. \n",
    "It helps computers understand, interpret, and manipulate human language. \n",
    "The applications of NLP are growing rapidly, from chatbots to translation services.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8103c48a",
   "metadata": {},
   "source": [
    "## 3. Tokenization\n",
    "\n",
    "Tokenization is the process of breaking down text into individual words (tokens) or sentences. NLTK provides two main functions for tokenization:\n",
    "- `word_tokenize()`: Splits text into words\n",
    "- `sent_tokenize()`: Splits text into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9cf5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence tokenization\n",
    "sentences = sent_tokenize(sample_text)\n",
    "print(\"Sentences:\")\n",
    "for i, sent in enumerate(sentences, 1):\n",
    "    print(f\"{i}. {sent}\")\n",
    "\n",
    "print(\"\\nWord tokenization:\")\n",
    "words = word_tokenize(sample_text)\n",
    "print(f\"Total words: {len(words)}\")\n",
    "print(\"First 20 words:\", words[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1719baab",
   "metadata": {},
   "source": [
    "## 4. Stopword Removal\n",
    "\n",
    "Stopwords are common words that typically don't contribute much to the meaning of a text (e.g., \"the\", \"is\", \"at\", \"which\"). Removing them can help focus on the important words in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b5ed40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Remove stopwords from our tokenized words\n",
    "words_no_stop = [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "print(\"Original words count:\", len(words))\n",
    "print(\"Words count after stopword removal:\", len(words_no_stop))\n",
    "print(\"\\nFirst 20 words without stopwords:\", words_no_stop[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4a861e",
   "metadata": {},
   "source": [
    "## 5. Stemming\n",
    "\n",
    "Stemming is the process of reducing words to their root or base form. For example, \"running\" becomes \"run\", \"fishes\" becomes \"fish\". NLTK provides several stemmers, and we'll use the Porter Stemmer, which is the most commonly used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fcd22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Porter Stemmer instance\n",
    "porter = PorterStemmer()\n",
    "\n",
    "# Stem the words\n",
    "stemmed_words = [porter.stem(word) for word in words_no_stop]\n",
    "\n",
    "# Compare original and stemmed words\n",
    "print(\"Original vs Stemmed words:\")\n",
    "for orig, stemmed in zip(words_no_stop[:10], stemmed_words[:10]):\n",
    "    print(f\"{orig:15} -> {stemmed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d511ec87",
   "metadata": {},
   "source": [
    "## 6. Lemmatization\n",
    "\n",
    "Lemmatization is similar to stemming but gives more meaningful results by using vocabulary and morphological analysis to return the dictionary base form of a word (lemma). For example, \"better\" becomes \"good\", while stemming might not handle such cases correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99948e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a WordNet Lemmatizer instance\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize words\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in words_no_stop]\n",
    "\n",
    "# Compare original, stemmed, and lemmatized words\n",
    "print(\"Original vs Stemmed vs Lemmatized words:\")\n",
    "for orig, stemmed, lemma in zip(words_no_stop[:10], stemmed_words[:10], lemmatized_words[:10]):\n",
    "    print(f\"{orig:15} -> {stemmed:15} -> {lemma}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc5610b",
   "metadata": {},
   "source": [
    "## 7. Part-of-Speech (POS) Tagging\n",
    "\n",
    "POS tagging is the process of marking up words in a text with their corresponding part of speech (noun, verb, adjective, etc.). This is useful for word sense disambiguation and further text analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cb5885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform POS tagging on the original words\n",
    "pos_tags = nltk.pos_tag(words)\n",
    "\n",
    "# Print the first 20 words with their POS tags\n",
    "print(\"Words with POS tags:\")\n",
    "for word, tag in pos_tags[:20]:\n",
    "    print(f\"{word:15} -> {tag}\")\n",
    "\n",
    "# Quick reference for common POS tags\n",
    "pos_reference = \"\"\"\n",
    "Common POS tags:\n",
    "- NN: Noun, singular\n",
    "- NNS: Noun, plural\n",
    "- VB: Verb, base form\n",
    "- VBD: Verb, past tense\n",
    "- JJ: Adjective\n",
    "- RB: Adverb\n",
    "- IN: Preposition\n",
    "- DT: Determiner\n",
    "\"\"\"\n",
    "print(pos_reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcf6294",
   "metadata": {},
   "source": [
    "## 8. Practice with Sample Text\n",
    "\n",
    "Let's put everything together and process a new piece of text using all the preprocessing steps we've learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cda107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New sample text\n",
    "new_text = \"\"\"Machine learning algorithms are becoming increasingly sophisticated. \n",
    "Researchers are developing new models daily, improving the ability of computers to learn from data. \n",
    "These advancements are revolutionizing many industries!\"\"\"\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenization\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Stopword removal\n",
    "    words_no_stop = [w for w in words if w.lower() not in stop_words]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatized = [lemmatizer.lemmatize(w) for w in words_no_stop]\n",
    "    \n",
    "    # POS tagging\n",
    "    pos_tags = nltk.pos_tag(lemmatized)\n",
    "    \n",
    "    return {\n",
    "        'original_words': words,\n",
    "        'words_no_stop': words_no_stop,\n",
    "        'lemmatized': lemmatized,\n",
    "        'pos_tags': pos_tags\n",
    "    }\n",
    "\n",
    "# Process the text\n",
    "results = preprocess_text(new_text)\n",
    "\n",
    "# Print results\n",
    "print(\"Original text:\")\n",
    "print(new_text)\n",
    "print(\"\\nProcessing results:\")\n",
    "print(f\"Original word count: {len(results['original_words'])}\")\n",
    "print(f\"After stopword removal: {len(results['words_no_stop'])}\")\n",
    "print(\"\\nFirst 10 words with their POS tags after preprocessing:\")\n",
    "for word, tag in results['pos_tags'][:10]:\n",
    "    print(f\"{word:15} -> {tag}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
